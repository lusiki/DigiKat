---
title: "Full data filter"
author: "Lux"
date: "2023-11-13"
output: html_document
---

```{r setup, include=T, message=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

 
```


```{r echo=F, eval=T, message=F , warning= FALSE, message=F}
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(writexl)
library(data.table)
library(stringi)
library(openxlsx)
library(RMySQL)
library(data.table)
library(stringr)
library(parallel)
library(pbapply)

```


```{r echo=F, eval=T, message=F , warning= FALSE, message=F}
source("./Codes/stemmer.R")
source("./Codes/text_analysis.R")
source("./Codes/write_tokens.R")
```

## IMPORT DATA

```{r echo=F, eval=T, message=F , warning= FALSE}
conn <- dbConnect(RMySQL::MySQL(), dbname = "determ_all", host = "127.0.0.1",
                  user = "Lux", password = "Theanswer0207", local_infile = TRUE)

query <- "SELECT * FROM media_space_2022"
data22 <- dbGetQuery(conn, query)
#close the connection
dbDisconnect(conn)

#fwrite(data22, "C:/Users/lukas/Desktop/dta22.xlsx")
# query <- "SELECT * FROM media_space_2022"
# # Execute the query and fetch the results
# data22 <- dbGetQuery(conn, query)

# Get the column names of the table
# column_names <- dbListFields(conn, "media_space_2022")
# 
# # Print the column names
# print(column_names)
# str(data22)
# summary(data22)


katolicki_izrazi <- read_excel("./Codes/katolički_izrazi.xlsx")
catholic_words <- katolicki_izrazi[order(katolicki_izrazi$name),]


katolicki_izrazi <- read_excel("./Codes/katolički_izrazi_small.xlsx")
catholic_words <- katolicki_izrazi[order(katolicki_izrazi$root),]

katolicki_izrazi <- read_excel("./Codes/katolicki_izrazi_.xlsx")
catholic_words <- katolicki_izrazi[order(katolicki_izrazi$word),]




```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Function to read all Excel files from a folder
read_all_excel_files <- function(folder_path) {
  
  # Get all Excel files in the folder
  excel_files <- list.files(
    path = folder_path,
    pattern = "\\.(xlsx|xls)$",
    full.names = TRUE,
    recursive = FALSE
  )
  
  if (length(excel_files) == 0) {
    stop("No Excel files found in the specified folder.")
  }
  
  cat("Found", length(excel_files), "Excel files\n")
  
  # Function to read a single Excel file
  read_single_file <- function(file_path) {
    tryCatch({
      # Read the first sheet (you can modify this to read specific sheets)
      df <- read_excel(file_path, sheet = 1)
      
      # Add filename as a column for tracking
      df$source_file <- basename(file_path)
      
      return(df)
    }, error = function(e) {
      cat("Error reading file:", file_path, "\n")
      cat("Error message:", e$message, "\n")
      return(NULL)
    })
  }
  
  # Determine number of cores for parallel processing
  num_cores <- min(detectCores() - 1, length(excel_files))
  
  cat("Using", num_cores, "cores for parallel processing\n")
  
  # Read files in parallel
  cl <- makeCluster(num_cores)
  clusterEvalQ(cl, {
    library(readxl)
  })
  
  # Process files in parallel
  file_list <- parLapply(cl, excel_files, read_single_file)
  
  # Stop cluster
  stopCluster(cl)
  
  # Remove NULL entries (failed reads)
  file_list <- file_list[!sapply(file_list, is.null)]
  
  if (length(file_list) == 0) {
    stop("No files could be read successfully.")
  }
  
  cat("Successfully read", length(file_list), "files\n")
  
  # Combine all data frames using data.table for speed
  combined_df <- rbindlist(file_list, fill = TRUE)
  
  # Convert back to data.frame if preferred
  combined_df <- as.data.frame(combined_df)
  
  cat("Combined data frame created with", nrow(combined_df), "rows and", ncol(combined_df), "columns\n")
  
  return(combined_df)
}

# Set your folder path
folder_path <- "C:/Users/Lukas/Dropbox/Determ_mediaspace/2021"


# Read all Excel files and create combined data frame
start_time <- Sys.time()
df_combined <- read_all_excel_files(folder_path)
end_time <- Sys.time()

cat("Processing completed in", round(difftime(end_time, start_time, units = "secs"), 2), "seconds\n")

# Display basic info about the combined data frame
cat("\nData frame summary:\n")
str(df_combined)
cat("\nFirst few rows:\n")
head(df_combined)

# Optional: Save the combined data frame
# write.csv(df_combined, "combined_excel_data.csv", row.names = FALSE)
# saveRDS(df_combined, "combined_excel_data.rds")  # More efficient for R objects
```


# MAKE REGEX PATTERN


```{r echo=F, eval=T, message=F , warning= FALSE}


catholic_words <- unique(tolower(katolicki_izrazi$word))
catholic_words_sorted <- catholic_words[order(-nchar(catholic_words))]

# Escape regex special characters in the roots
catholic_words_escaped <- str_replace_all(
  catholic_words_sorted,
  "([\\.\\|\\(\\)\\[\\]\\{\\}\\\\\\+\\*\\?\\^\\$])",
  "\\\\\\1"
)

# Separate multi-word roots (contain spaces) and single-word roots
multi_word_roots <- catholic_words_escaped[grepl(" ", catholic_words_sorted)]
single_word_roots <- catholic_words_escaped[!grepl(" ", catholic_words_sorted)]


# Create pattern for single-word roots with word roots
pattern_single <- if(length(single_word_roots) > 0) {
  paste0("\\b(", paste(single_word_roots, collapse = "|"), ")\\w*\\b")
} else {
  NULL
}

# Create pattern for multi-word roots (exact matches)
pattern_multi <- if(length(multi_word_roots) > 0) {
  paste0("\\b(", paste(multi_word_roots, collapse = "|"), ")\\b")
} else {
  NULL
}

# Combine single-word and multi-word patterns with preceding space or start of string
if(!is.null(pattern_single) & !is.null(pattern_multi)) {
  # Ensure that each pattern is preceded by a space or start of string
  pattern_final <- paste0("(?:(?<=\\s)|(?<=^))(", 
                          paste(c(single_word_roots, multi_word_roots), collapse = "|"), 
                          ")\\w*\\b")
} else if(!is.null(pattern_single)) {
  pattern_final <- paste0("(?:(?<=\\s)|(?<=^))(", 
                          paste(single_word_roots, collapse = "|"), 
                          ")\\w*\\b")
} else {
  pattern_final <- paste0("(?:(?<=\\s)|(?<=^))(", 
                          paste(multi_word_roots, collapse = "|"), 
                          ")\\b")
}

# Display the final regex pattern
print(pattern_final)
# Example Output:
# "(?:(?<=\\s)|(?<=^))(apostolsk|ateist|biskup|biskupij|blagoslov|blagoslovi|blagoslovljen)\\w*\\b"



```



```{r echo=F, eval=T, message=F , warning= FALSE}


# 1. Put all your base terms in a character vector:
terms <- c(
  "presveti oltarski sakrament", "apostolsko nasljeđe", "bolesničko pomazanje",
  "bezgrešno začeće", "nicejsko vjerovanje", "vazmeno trodnevlje",
  "rimokatolička crkva", "sveto trojstvo", "neokaljano srce",
  "katolička crkva", "sveta krunica", "sveta stolica", "sveto pismo",
  "časna sestra", "časne sestre", "srce isusovo", "križni put",
  "sveti otac", "sveti križ", "sveti red", "opus dei", "nadbiskupija",
  "ispovjedaonica", "franjevac", "benediktinac", "dominikanac",
  "salezijanac", "karmelićanin", "kršćanstvo", "hodočašće", "pontifikat",
  "vatikanski", "euharistija", "ispovijed", "katekizam", "liturgija",
  "redovnica", "vjeronauk", "enciklika", "gvardijan", "kardinal",
  "nadbiskup", "provincijal", "sakrament", "svećenik", "teologija",
  "vatikan", "biskupija", "blagoslov", "celibat", "hodočasnik",
  "isusovac", "kapelan", "katedrala", "kršćanin", "monsinjor",
  "papinski", "posvećenje", "redovnik", "relikvija", "stepinac",
  "teološki", "ukazanje", "zaređenje", "bazilika", "crkveni",
  "jezuiti", "kapitul", "korizma", "krunica", "papinstvo", "pobožnost",
  "pontif", "pričest", "trapist", "časna", "đakon", "došašće",
  "hostija", "kaptol", "krizma", "nuncij", "pavlin",
  "biskup", "gospa", "kler", "misa", "papa", "demon", "duhovnik", 
  "duhovnost", "evanđelje", "franjevci", "katekizam", "križ"
  
)

# 2. Build the regex string, escaping backslashes for R:
pattern_final <- sprintf(
  "(?:(?<=\\s)|(?<=^))(%s)\\w*\\b",
  paste(terms, collapse = "|")
)

# 3. That’s it:
```



# MATCHING PROCEDURE
```{r echo=F, eval=T, message=F , warning= FALSE}
setDT(data22)

filtered_df <- data22 %>%
  head(1000) %>%
  mutate(
    # Extract all matches based on the regex pattern
    matches = str_match_all(TITLE, regex(pattern_final, ignore_case = TRUE))
  ) %>%
  
  mutate(
    # Extract the captured group (matched words without preceding space)
    matched_words = sapply(matches, function(x) {
      # x is a matrix with full match and captured group
      # We extract the second column which contains the captured group
      matched <- x[,2]
      # Append any suffixes (the pattern already captures root and suffix)
      # Remove NA values
      matched <- matched[!is.na(matched)]
      # Return unique matched words separated by commas
      paste(unique(matched), collapse = ", ")
    })
  ) %>%
  
  mutate(
    # Count the number of unique matched words
    match_count = sapply(str_split(matched_words, ", "), length)
  ) %>%
  
  # Filter rows where the number of matches is two or more
  filter(match_count >= 2) %>%
  
  # Optionally, remove the auxiliary columns
  select(-matches, -match_count)
```


```{r echo=F, eval=T, message=F , warning= FALSE}
filtered_df_ <- data22 %>%
  # Optionally, limit to the first 100,000 rows
#  head(100000) %>%
  
  # Extract all matches based on the regex pattern
  mutate(
    matches = str_match_all(FULL_TEXT, regex(pattern_final, ignore_case = TRUE))
  ) %>%
  
  # Extract matched roots and full words
  mutate(
    # Extract the captured group (matched roots)
    matched_roots = sapply(matches, function(x) {
      # x is a matrix with full match and captured group
      # Extract the second column which contains the captured group
      matched <- x[,2]
      # Remove NA values
      matched <- matched[!is.na(matched)]
      # Return unique matched roots separated by commas
      paste(unique(matched), collapse = ", ")
    }),
    # Extract the full matched words
    matched_full_words = sapply(matches, function(x) {
      # Extract the first column which contains the full match
      full_matched <- x[,1]
      # Remove NA values
      full_matched <- full_matched[!is.na(full_matched)]
      # Return unique matched full words separated by commas
      paste(unique(full_matched), collapse = ", ")
    })
  ) %>%
  
  # Count the number of unique matched full words
  mutate(
    match_count = sapply(str_split(matched_full_words, ", "), length)
  ) %>%
  
  # Filter rows where the number of matches is two or more
  filter(match_count >= 3) %>%
  
  # Optionally, remove the auxiliary columns
  select(-matches, -match_count)

# View the filtered data with matched roots and full words
print(filtered_df)
```



```{r}

# Optimized text processing function with progress tracking
# Install required packages if not already installed
if (!require("dplyr")) install.packages("dplyr")
if (!require("stringr")) install.packages("stringr")
if (!require("parallel")) install.packages("parallel")
if (!require("data.table")) install.packages("data.table")
if (!require("pbapply")) install.packages("pbapply")

library(dplyr)
library(stringr)
library(parallel)
library(data.table)
library(pbapply)

# Optimized text processing function
process_text_with_progress <- function(data, text_column = "FULL_TEXT", pattern_final, 
                                     min_matches = 2, chunk_size = 10000, 
                                     use_parallel = TRUE, verbose = TRUE) {
  
  # Start timing
  start_time <- Sys.time()
  total_rows <- nrow(data)
  
  if (verbose) {
    cat("Starting text processing...\n")
    cat("Total rows to process:", total_rows, "\n")
    cat("Minimum matches required:", min_matches, "\n")
    cat("Chunk size:", chunk_size, "\n")
  }
  
  # Convert to data.table for faster operations
  if (!is.data.table(data)) {
    data <- as.data.table(data)
  }
  
  # Pre-compile regex pattern for better performance
  compiled_pattern <- regex(pattern_final, ignore_case = TRUE)
  
  # Function to process a single chunk
  process_chunk <- function(chunk_data, chunk_num = NULL) {
    if (!is.null(chunk_num) && verbose) {
      cat("Processing chunk", chunk_num, "...\n")
    }
    
    # Extract all matches using vectorized operations
    matches_list <- str_match_all(chunk_data[[text_column]], compiled_pattern)
    
    # Process matches more efficiently
    chunk_results <- data.table(
      row_idx = seq_len(nrow(chunk_data)),
      matched_roots = character(nrow(chunk_data)),
      matched_full_words = character(nrow(chunk_data)),
      match_count = integer(nrow(chunk_data))
    )
    
    # Vectorized processing of matches
    for (i in seq_along(matches_list)) {
      match_matrix <- matches_list[[i]]
      
      if (nrow(match_matrix) > 0) {
        # Extract captured groups (roots) - column 2
        matched_roots <- match_matrix[, 2]
        matched_roots <- matched_roots[!is.na(matched_roots)]
        
        # Extract full matches - column 1
        matched_full <- match_matrix[, 1]
        matched_full <- matched_full[!is.na(matched_full)]
        
        if (length(matched_roots) > 0) {
          chunk_results$matched_roots[i] <- paste(unique(matched_roots), collapse = ", ")
          chunk_results$matched_full_words[i] <- paste(unique(matched_full), collapse = ", ")
          chunk_results$match_count[i] <- length(unique(matched_full))
        }
      }
    }
    
    # Filter rows with sufficient matches
    valid_rows <- chunk_results$match_count >= min_matches
    
    if (any(valid_rows)) {
      # Combine with original data
      result_chunk <- cbind(chunk_data[valid_rows], 
                           chunk_results[valid_rows, .(matched_roots, matched_full_words)])
      return(result_chunk)
    } else {
      return(NULL)
    }
  }
  
  # Function to process chunks in parallel
  process_parallel <- function(chunk_indices) {
    # Setup cluster
    num_cores <- min(detectCores() - 1, length(chunk_indices))
    if (verbose) cat("Using", num_cores, "cores for parallel processing\n")
    
    cl <- makeCluster(num_cores)
    clusterEvalQ(cl, {
      library(data.table)
      library(stringr)
    })
    
    # Export variables to cluster
    clusterExport(cl, c("data", "text_column", "compiled_pattern", "min_matches", 
                       "process_chunk"), envir = environment())
    
    # Process chunks in parallel with progress bar
    chunk_results <- pblapply(chunk_indices, function(i) {
      start_idx <- (i - 1) * chunk_size + 1
      end_idx <- min(i * chunk_size, total_rows)
      chunk_data <- data[start_idx:end_idx]
      return(process_chunk(chunk_data, i))
    }, cl = cl)
    
    stopCluster(cl)
    return(chunk_results)
  }
  
  # Function to process chunks sequentially
  process_sequential <- function(chunk_indices) {
    chunk_results <- list()
    
    for (i in chunk_indices) {
      start_idx <- (i - 1) * chunk_size + 1
      end_idx <- min(i * chunk_size, total_rows)
      chunk_data <- data[start_idx:end_idx]
      
      # Progress update
      if (verbose) {
        progress_pct <- round((end_idx / total_rows) * 100, 1)
        elapsed_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
        estimated_total <- elapsed_time / (end_idx / total_rows)
        remaining_time <- estimated_total - elapsed_time
        
        cat(sprintf("Progress: %d/%d rows (%.1f%%) | Elapsed: %.1fs | ETA: %.1fs\n", 
                   end_idx, total_rows, progress_pct, elapsed_time, remaining_time))
      }
      
      chunk_results[[i]] <- process_chunk(chunk_data, i)
    }
    
    return(chunk_results)
  }
  
  # Calculate number of chunks
  num_chunks <- ceiling(total_rows / chunk_size)
  chunk_indices <- 1:num_chunks
  
  if (verbose) {
    cat("Processing in", num_chunks, "chunks\n")
  }
  
  # Process chunks
  if (use_parallel && num_chunks > 1) {
    chunk_results <- process_parallel(chunk_indices)
  } else {
    chunk_results <- process_sequential(chunk_indices)
  }
  
  # Combine results
  if (verbose) cat("Combining results...\n")
  
  # Remove NULL results
  chunk_results <- chunk_results[!sapply(chunk_results, is.null)]
  
  if (length(chunk_results) == 0) {
    if (verbose) cat("No matches found meeting the criteria.\n")
    return(data.table())
  }
  
  # Combine all chunks efficiently
  final_result <- rbindlist(chunk_results, fill = TRUE)
  
  # Convert back to data.frame if input was data.frame
  if (!is.data.table(data)) {
    final_result <- as.data.frame(final_result)
  }
  
  # Final timing and statistics
  end_time <- Sys.time()
  total_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  if (verbose) {
    cat("\n=== PROCESSING COMPLETE ===\n")
    cat("Total processing time:", round(total_time, 2), "seconds\n")
    cat("Rows processed:", total_rows, "\n")
    cat("Rows matching criteria:", nrow(final_result), "\n")
    cat("Match rate:", round((nrow(final_result) / total_rows) * 100, 2), "%\n")
    cat("Processing speed:", round(total_rows / total_time, 0), "rows/second\n")
  }
  
  return(final_result)
}

# Example usage with your data
# Make sure to define your pattern_final variable before running
# pattern_final <- "your_regex_pattern_here"



proba <- head(df_combined,100)


# Run the optimized function
filtered_df_ <- process_text_with_progress(
  data = df_combined,
  text_column = "FULL_TEXT",
  pattern_final = pattern_final,  # Make sure this is defined
  min_matches = 2,
  chunk_size = 10000,  # Adjust based on your memory
  use_parallel = TRUE,
  verbose = TRUE
)

# Alternative: Process with different chunk sizes based on data size
auto_chunk_size <- function(total_rows, available_memory_gb = 4) {
  # Estimate optimal chunk size based on available memory
  # Assuming each row takes about 1KB on average
  max_chunk_size <- (available_memory_gb * 1024 * 1024) / 1  # Convert to KB
  optimal_chunk <- min(max_chunk_size, max(1000, total_rows / 100))
  return(as.integer(optimal_chunk))
}

# For very large datasets, use adaptive chunk sizing
if (nrow(df_combined) > 100000) {
  optimal_chunk <- auto_chunk_size(nrow(df_combined))
  cat("Using adaptive chunk size:", optimal_chunk, "\n")
  
  filtered_df_ <- process_text_with_progress(
    data = df_combined,
    text_column = "FULL_TEXT",
    pattern_final = pattern_final,
    min_matches = 2,
    chunk_size = optimal_chunk,
    use_parallel = TRUE,
    verbose = TRUE
  )
}
```



```{r}
#Memory-efficient text processing function for very large datasets
# Install required packages if not already installed
required_packages <- c("dplyr", "stringr", "data.table", "future", "future.apply", "progressr")
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

# Optimized function for very large datasets
process_text_large_dataset <- function(data, text_column = "FULL_TEXT", pattern_final, 
                                     min_matches = 2, chunk_size = 5000, 
                                     use_parallel = TRUE, verbose = TRUE, 
                                     save_intermediate = TRUE, temp_dir = tempdir()) {
  
  # Start timing
  start_time <- Sys.time()
  total_rows <- nrow(data)
  
  if (verbose) {
    cat("Starting text processing for large dataset...\n")
    cat("Total rows to process:", format(total_rows, big.mark = ","), "\n")
    cat("Minimum matches required:", min_matches, "\n")
    cat("Chunk size:", format(chunk_size, big.mark = ","), "\n")
    cat("Save intermediate results:", save_intermediate, "\n")
  }
  
  # Convert to data.table for faster operations
  if (!is.data.table(data)) {
    data <- as.data.table(data)
  }
  
  # Pre-compile regex pattern for better performance
  compiled_pattern <- regex(pattern_final, ignore_case = TRUE)
  
  # Function to process a single chunk (standalone, no external dependencies)
  process_chunk_standalone <- function(chunk_data, pattern_str, text_col, min_match, chunk_id) {
    # Load required libraries within function
    library(stringr)
    library(data.table)
    
    # Recreate pattern within function
    pattern_obj <- regex(pattern_str, ignore_case = TRUE)
    
    # Extract all matches
    matches_list <- str_match_all(chunk_data[[text_col]], pattern_obj)
    
    # Initialize results
    n_rows <- nrow(chunk_data)
    matched_roots <- character(n_rows)
    matched_full_words <- character(n_rows)
    match_count <- integer(n_rows)
    
    # Process matches
    for (i in seq_along(matches_list)) {
      match_matrix <- matches_list[[i]]
      
      if (nrow(match_matrix) > 0) {
        # Extract captured groups (roots) - column 2
        roots <- match_matrix[, 2]
        roots <- roots[!is.na(roots)]
        
        # Extract full matches - column 1
        full_matches <- match_matrix[, 1]
        full_matches <- full_matches[!is.na(full_matches)]
        
        if (length(roots) > 0) {
          matched_roots[i] <- paste(unique(roots), collapse = ", ")
          matched_full_words[i] <- paste(unique(full_matches), collapse = ", ")
          match_count[i] <- length(unique(full_matches))
        }
      }
    }
    
    # Filter rows with sufficient matches
    valid_rows <- match_count >= min_match
    
    if (any(valid_rows)) {
      result <- chunk_data[valid_rows]
      result$matched_roots <- matched_roots[valid_rows]
      result$matched_full_words <- matched_full_words[valid_rows]
      return(result)
    } else {
      return(NULL)
    }
  }
  
  # Calculate number of chunks
  num_chunks <- ceiling(total_rows / chunk_size)
  
  if (verbose) {
    cat("Processing in", format(num_chunks, big.mark = ","), "chunks\n")
  }
  
  # Setup progress tracking
  if (verbose) {
    cat("Setting up progress tracking...\n")
  }
  
  # Create temporary directory for intermediate results if needed
  if (save_intermediate) {
    temp_results_dir <- file.path(temp_dir, paste0("text_processing_", Sys.getpid()))
    dir.create(temp_results_dir, showWarnings = FALSE)
    if (verbose) cat("Temp directory:", temp_results_dir, "\n")
  }
  
  # Process chunks with proper memory management
  if (use_parallel && num_chunks > 1) {
    # Setup future for parallel processing
    if (verbose) cat("Setting up parallel processing...\n")
    
    # Use multisession to avoid serialization issues
    plan(multisession, workers = min(7, num_chunks))
    
    # Process chunks in batches to avoid memory issues
    batch_size <- 50  # Process 50 chunks at a time
    num_batches <- ceiling(num_chunks / batch_size)
    all_results <- list()
    
    for (batch_idx in 1:num_batches) {
      start_chunk <- (batch_idx - 1) * batch_size + 1
      end_chunk <- min(batch_idx * batch_size, num_chunks)
      
      if (verbose) {
        cat(sprintf("Processing batch %d/%d (chunks %d-%d)\n", 
                   batch_idx, num_batches, start_chunk, end_chunk))
      }
      
      # Process batch
      batch_results <- future_lapply(start_chunk:end_chunk, function(i) {
        start_idx <- (i - 1) * chunk_size + 1
        end_idx <- min(i * chunk_size, total_rows)
        chunk_data <- data[start_idx:end_idx]
        
        result <- process_chunk_standalone(chunk_data, pattern_final, text_column, min_matches, i)
        
        # Save intermediate result if requested
        if (save_intermediate && !is.null(result)) {
          saveRDS(result, file.path(temp_results_dir, paste0("chunk_", i, ".rds")))
        }
        
        return(result)
      })
      
      # Store batch results
      all_results <- c(all_results, batch_results)
      
      # Progress update
      if (verbose) {
        progress_pct <- round((end_chunk / num_chunks) * 100, 1)
        elapsed_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
        estimated_total <- elapsed_time / (end_chunk / num_chunks)
        remaining_time <- estimated_total - elapsed_time
        
        cat(sprintf("Batch %d complete. Overall progress: %.1f%% | Elapsed: %.1fs | ETA: %.1fs\n", 
                   batch_idx, progress_pct, elapsed_time, remaining_time))
      }
      
      # Force garbage collection
      gc()
    }
    
    chunk_results <- all_results
    
  } else {
    # Sequential processing with progress tracking
    chunk_results <- list()
    
    for (i in 1:num_chunks) {
      start_idx <- (i - 1) * chunk_size + 1
      end_idx <- min(i * chunk_size, total_rows)
      chunk_data <- data[start_idx:end_idx]
      
      result <- process_chunk_standalone(chunk_data, pattern_final, text_column, min_matches, i)
      chunk_results[[i]] <- result
      
      # Save intermediate result if requested
      if (save_intermediate && !is.null(result)) {
        saveRDS(result, file.path(temp_results_dir, paste0("chunk_", i, ".rds")))
      }
      
      # Progress update
      if (verbose && i %% 10 == 0) {
        progress_pct <- round((end_idx / total_rows) * 100, 1)
        elapsed_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
        estimated_total <- elapsed_time / (end_idx / total_rows)
        remaining_time <- estimated_total - elapsed_time
        
        cat(sprintf("Progress: %s/%s rows (%.1f%%) | Elapsed: %.1fs | ETA: %.1fs\n", 
                   format(end_idx, big.mark = ","), format(total_rows, big.mark = ","), 
                   progress_pct, elapsed_time, remaining_time))
      }
      
      # Force garbage collection every 50 chunks
      if (i %% 50 == 0) {
        gc()
      }
    }
  }
  
  # Combine results
  if (verbose) cat("Combining results...\n")
  
  # Remove NULL results
  chunk_results <- chunk_results[!sapply(chunk_results, is.null)]
  
  if (length(chunk_results) == 0) {
    if (verbose) cat("No matches found meeting the criteria.\n")
    return(data.table())
  }
  
  # Combine all chunks efficiently
  final_result <- rbindlist(chunk_results, fill = TRUE)
  
  # Clean up temporary files
  if (save_intermediate && dir.exists(temp_results_dir)) {
    if (verbose) cat("Cleaning up temporary files...\n")
    unlink(temp_results_dir, recursive = TRUE)
  }
  
  # Convert back to data.frame if input was data.frame
  if (!is.data.table(data)) {
    final_result <- as.data.frame(final_result)
  }
  
  # Final timing and statistics
  end_time <- Sys.time()
  total_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  if (verbose) {
    cat("\n=== PROCESSING COMPLETE ===\n")
    cat("Total processing time:", round(total_time, 2), "seconds\n")
    cat("Rows processed:", format(total_rows, big.mark = ","), "\n")
    cat("Rows matching criteria:", format(nrow(final_result), big.mark = ","), "\n")
    cat("Match rate:", round((nrow(final_result) / total_rows) * 100, 2), "%\n")
    cat("Processing speed:", format(round(total_rows / total_time, 0), big.mark = ","), "rows/second\n")
  }
  
  return(final_result)
}

# Alternative: Process in smaller sequential batches (most reliable for very large datasets)
process_text_sequential_batches <- function(data, text_column = "FULL_TEXT", pattern_final, 
                                          min_matches = 2, chunk_size = 2000, 
                                          verbose = TRUE, save_progress = TRUE) {
  
  start_time <- Sys.time()
  total_rows <- nrow(data)
  
  if (verbose) {
    cat("Starting sequential batch processing...\n")
    cat("Total rows:", format(total_rows, big.mark = ","), "\n")
    cat("Chunk size:", format(chunk_size, big.mark = ","), "\n")
  }
  
  # Convert to data.table
  if (!is.data.table(data)) {
    data <- as.data.table(data)
  }
  
  compiled_pattern <- regex(pattern_final, ignore_case = TRUE)
  num_chunks <- ceiling(total_rows / chunk_size)
  all_results <- list()
  
  for (i in 1:num_chunks) {
    start_idx <- (i - 1) * chunk_size + 1
    end_idx <- min(i * chunk_size, total_rows)
    chunk_data <- data[start_idx:end_idx]
    
    # Process chunk
    matches_list <- str_match_all(chunk_data[[text_column]], compiled_pattern)
    
    n_rows <- nrow(chunk_data)
    matched_roots <- character(n_rows)
    matched_full_words <- character(n_rows)
    match_count <- integer(n_rows)
    
    for (j in seq_along(matches_list)) {
      match_matrix <- matches_list[[j]]
      if (nrow(match_matrix) > 0) {
        roots <- match_matrix[, 2][!is.na(match_matrix[, 2])]
        full_matches <- match_matrix[, 1][!is.na(match_matrix[, 1])]
        
        if (length(roots) > 0) {
          matched_roots[j] <- paste(unique(roots), collapse = ", ")
          matched_full_words[j] <- paste(unique(full_matches), collapse = ", ")
          match_count[j] <- length(unique(full_matches))
        }
      }
    }
    
    # Filter and store results
    valid_rows <- match_count >= min_matches
    if (any(valid_rows)) {
      result <- chunk_data[valid_rows]
      result$matched_roots <- matched_roots[valid_rows]
      result$matched_full_words <- matched_full_words[valid_rows]
      all_results[[i]] <- result
    }
    
    # Progress update
    if (verbose && i %% 10 == 0) {
      progress_pct <- round((end_idx / total_rows) * 100, 1)
      elapsed_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
      estimated_total <- elapsed_time / (end_idx / total_rows)
      remaining_time <- estimated_total - elapsed_time
      
      cat(sprintf("Chunk %d/%d (%.1f%%) | Elapsed: %.1fs | ETA: %.1fs\n", 
                 i, num_chunks, progress_pct, elapsed_time, remaining_time))
    }
    
    # Memory management
    if (i %% 100 == 0) {
      gc()
    }
  }
  
  # Combine results
  if (verbose) cat("Combining final results...\n")
  final_result <- rbindlist(all_results[!sapply(all_results, is.null)], fill = TRUE)
  
  end_time <- Sys.time()
  total_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  if (verbose) {
    cat("\n=== PROCESSING COMPLETE ===\n")
    cat("Total time:", round(total_time, 2), "seconds\n")
    cat("Rows processed:", format(total_rows, big.mark = ","), "\n")
    cat("Rows matching:", format(nrow(final_result), big.mark = ","), "\n")
    cat("Speed:", format(round(total_rows / total_time, 0), big.mark = ","), "rows/second\n")
  }
  
  return(final_result)
}

# RECOMMENDED USAGE FOR YOUR LARGE DATASET (3M+ rows):

# Option 1: Sequential processing (most reliable)
cat("Starting sequential processing (recommended for stability)...\n")
filtered_df_ <- process_text_sequential_batches(
  data = df_combined,
  text_column = "FULL_TEXT",
  pattern_final = pattern_final,
  min_matches = 2,
  chunk_size = 2000,  # Smaller chunks for stability
  verbose = TRUE
)





saveRDS(filtered_df_, "D:/LUKA/Dropbox/filtered_df_2021.rds")


saveRDS(dta, "D:/LUKA/Dropbox/CCDMS.rds")
# save as csv
write.csv(dta, "D:/LUKA/Dropbox/CCDMS.csv", row.names = FALSE)


```





















```{r}
# show me decsciprive stats, count per FROM_SITE, SOURCE_TYPE, AUTHOR

filtered_df_ %>%
  group_by(FROM_SITE) %>%
  summarise(
    count = n(),
    unique_sources = n_distinct(SOURCE_TYPE),
    unique_authors = n_distinct(AUTHOR)
  ) %>%
  arrange(desc(count))



filtered_df_ %>%
  group_by(SOURCE_TYPE) %>%
  summarise(
    count = n(),
    unique_sites = n_distinct(FROM_SITE),
    unique_authors = n_distinct(AUTHOR)
  ) %>%
  arrange(desc(count))


filtered_df %>% 
  group_by(AUTHOR) %>%
  summarise(
    count = n(),
    unique_sites = n_distinct(FROM_SITE),
    unique_sources = n_distinct(SOURCE_TYPE)
  ) %>%
  arrange(desc(count))

```

```{r}

filtered_df_ %>%
  filter(SOURCE_TYPE == "web") %>%
  group_by(FROM) %>%
  summarise(
    count = n(),
    unique_sources = n_distinct(SOURCE_TYPE),
    unique_authors = n_distinct(AUTHOR)
  ) %>%
  arrange(desc(count))
  


```




```{r}

web  <- filtered_df_ %>%
  filter(SOURCE_TYPE == "web")


web %>%
  filter(FROM_SITE == "vecernji.hr") %>%
  View()

# Unnest tokens using tidytext
tidy_text <- filtered_df_ %>%
  unnest_tokens(word, TITLE)

tidy_text <- tidy_text %>%
  anti_join(stop_corpus, by = "word") %>%
  # remove numbers
  filter(!grepl("[0-9]", word)) 


tidy_text %>%
  count(word, sort = TRUE) %>%
  head(200) %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE) 


tidy_text %>%
  count(word, sort = TRUE) %>%
  head(200) %>% write.xlsx(., "./Codes/katolicki_izrazi_.xlsx")

write.xlsx(filtered_df_, "C:/Users/lukas/Desktop/filtered_df_.xlsx")

```



```{r}

```


## SAMPLE TO SIZE

```{r}

set.seed(123) # For reproducibility

# Sample 1 million observations; adjust as needed based on your system's capacity
sample_size <- 10000

data22 <- as.data.table(data22) 
data22_ <- data22[sample(.N, sample_size)]
#rm(data22) # Remove the original data to free up memory
```








## PREPROCESSING

```{r}
# Remove duplicate entries based on the 'URL' column
data_sample <- unique(data_sample, by = "URL")

# Combine relevant text fields into a single column
data_sample <- data_sample %>%
  mutate(
    combined_text = paste(
      coalesce(as.character(TITLE), ""),
      coalesce(as.character(FULL_TEXT), ""),
      sep = " "
    )
  ) %>%
  # Remove rows where 'combined_text' is empty
  filter(str_trim(combined_text) != "")

```



## TOKENIZATION AND CLEANING
 

```{r}

katolicki_izrazi <- read_excel("./Codes/katolički_izrazi.xlsx")
catholic_words <- katolicki_izrazi[order(katolicki_izrazi$name),]

#katolicki_izrazi <- rbind(katolicki_izrazi, generalno)
#katolicki_izrazi <- unique(katolicki_izrazi)
#write.xlsx(katolicki_izrazi, "./Codes/katolički_izrazi.xlsx")


# Select necessary columns for tokenization
text_data <- data_sample %>%
  select(URL, combined_text)

# Unnest tokens using tidytext
tidy_text <- text_data %>%
  unnest_tokens(word, combined_text)

tidy_text <- tidy_text %>%
  anti_join(stop_corpus, by = "word") %>%
  # remove numbers
  filter(!grepl("[0-9]", word)) 
# 
# tidy_text %>%
#   count(word, sort = TRUE) %>%
#   head(100) %>%
#   kable() %>%
#   kable_styling("striped", full_width = FALSE)
```

## STEMMING

```{r}
# Apply stemming to the "word" column
tidy_text_ <- tidy_text %>%
  mutate(
    stemmed_word = sapply(word, function(w) {
      stemmed = write_tokens(w)  # Apply your write_tokens function
      # Process the result similar to your generalno example
      stemmed = sapply(strsplit(stemmed, "\t"), `[`, 2)  # Extract the second element
      stemmed
    })
  )

# Enframe the results if necessary
tidy_text_ <- tidy_text_ %>%
  rename(original_word = word) %>%
  select(URL, original_word, stemmed_word)
```


```{r}
process_in_batches <- function(data, batch_size = 1000) {
  total_rows <- nrow(data)
  num_batches <- ceiling(total_rows / batch_size)
  
  # Initialize a new column to store stemmed words
  data$stemmed_word <- NA
  
  # Initialize variables to track time
  start_time <- Sys.time()
  batch_times <- numeric(num_batches)  # To store time taken for each batch
  
  for (i in seq_len(num_batches)) {
    # Calculate the range for the current batch
    start_row <- (i - 1) * batch_size + 1
    end_row <- min(i * batch_size, total_rows)
    
    # Record the start time for the current batch
    batch_start_time <- Sys.time()
    
    # Process the current batch
    current_batch <- data[start_row:end_row, ]
    
    data$stemmed_word[start_row:end_row] <- sapply(current_batch$word, function(w) {
      # Apply the write_tokens function
      stemmed <- write_tokens(w)  # Apply your write_tokens function
      # Extract the second element from the token result
      stemmed <- sapply(strsplit(stemmed, "\t"), `[`, 2)
      stemmed
    })
    
    # Record the end time for the current batch
    batch_end_time <- Sys.time()
    
    # Calculate time taken for the current batch in seconds
    batch_time <- as.numeric(difftime(batch_end_time, batch_start_time, units = "secs"))
    batch_times[i] <- batch_time
    
    # Calculate elapsed time
    elapsed_time <- as.numeric(difftime(batch_end_time, start_time, units = "secs"))
    
    # Estimate total time based on average time per batch
    average_time_per_batch <- mean(batch_times[1:i])
    estimated_total_time <- average_time_per_batch * num_batches
    estimated_remaining_time <- estimated_total_time - elapsed_time
    
    # Format time for readability
    format_time <- function(seconds) {
      sprintf("%02dh:%02dm:%02ds",
              floor(seconds / 3600),
              floor((seconds %% 3600) / 60),
              floor(seconds %% 60))
    }
    
    # Print progress with time information
    cat(sprintf("Processed batch %d/%d (Rows %d to %d) | Batch Time: %s | Elapsed Time: %s | Estimated Remaining Time: %s\n",
                i, num_batches, start_row, end_row,
                format_time(batch_time),
                format_time(elapsed_time),
                format_time(max(0, estimated_remaining_time))))
  }
  
  # Calculate total processing time
  total_processing_time <- Sys.time() - start_time
  cat(sprintf("Completed processing %d rows in %s.\n",
              total_rows,
              format_time(as.numeric(total_processing_time, units = "secs"))))
  
  return(data)
}

# Apply the batch processing function to tidy_text
batch_size <- 1000  # Adjust the batch size as needed
tidy_text_ <- process_in_batches(tidy_text, batch_size)
```







```{r}
# filter rows in tidy_text that contain the word in root in katolicki_izrazi

katolcki_redovi <- tidy_text_ %>%
  filter(stemmed_word %in% katolicki_izrazi$root) %>%
  select(URL, word, stemmed_word)

katolcki_urlovi <- katolcki_redovi %>%
  distinct(URL, .keep_all = TRUE)

katolcki_clanci <- data22 %>%
  filter(URL %in% katolcki_urlovi$URL)
# add a column to katolcki_clanci that contains all words in the root column that are related to the URL



```


# Preform stemming





```{r}
tidy_text %>%
  count(word, sort = TRUE) %>%
  head(100) %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE)
```



















































```{r echo=F, eval=T, message=F , warning= FALSE}

range(data22$DATE)

#filter only web from column FROM and make a count number of rows per 
data

data22 %>%
  filter(SOURCE_TYPE == "web") %>%
  # filter only year 2024
  filter(year(DATE) == 2024) %>%
  group_by(FROM_SITE) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) %>%
  mutate(Percentage = (Count / sum(Count)) * 100, # Calculate percentage
         Cumulative_Percentage = cumsum(Percentage)) %>% # Calculate cumulative percentage
  head(25) %>%
  pull(FROM_SITE) -> top20

```

```{r}


top25 <- data22 %>%
  filter(SOURCE_TYPE == "web") %>%          # Filter for SOURCE_TYPE "web"
  filter(year(DATE) == 2022) %>%            # Filter for the year 2024
  group_by(FROM_SITE) %>%                    # Group by FROM_SITE
  summarise(Count = n(), .groups = 'drop') %>% # Count occurrences and drop grouping
  arrange(desc(Count)) %>%                   # Arrange in descending order of Count
  mutate(
    Percentage = (Count / sum(Count)) * 100,  # Calculate percentage
    Cumulative_Percentage = cumsum(Percentage) # Calculate cumulative percentage
  ) %>%
  slice_head(n = 25) %>%                     # Select the top 25
  pull(FROM_SITE)   


not_top25 <- data22 %>%
  filter(SOURCE_TYPE == "web")%>%
  filter(year(DATE) == 2022) %>%               # Ensure only year 2024 is considered
  filter(!(FROM_SITE %in% top25)) %>%                     # Select the top 25
  pull(FROM_SITE)  



# filter data24 for dta24_filtered


dta22_filter <- data22 %>%
  filter(SOURCE_TYPE == "web") %>%          # Filter for SOURCE_TYPE "web"
  filter(year(DATE) == 2022) %>%            # Filter for the year 2024
  filter(FROM_SITE %in% not_top25)    # Exclude sites in top25




# Exclude sites in top25

```


```{r echo=F, eval=F, message=F , warning= FALSE}


  
# remove last two columns

dta22_filter <- dta22_filter %>% select(-c(46,47))

proba1 =
  dta22_filter %>%
  slice(1:100000) 

proba2 =
  dta22_filter %>%
  slice(100001:200000)

proba3 =
  dta22_filter %>%
  slice(200001:300000)
proba4 =
  dta22_filter %>%
  slice(300001:400000)
proba5 =
  dta22_filter %>%
  slice(400001:500000)
proba6 =
  dta22_filter %>%
  slice(500001:600000)
proba7 =
  dta22_filter %>%
  slice(600001:700000)
proba8 = 
  dta22_filter %>%
  slice(700001:800000)

proba9 = 
  dta22_filter %>%
  slice(800001:900000)

proba10 = 
  dta22_filter %>%
  slice(900001:1000000)
proba11 = 
  dta22_filter %>%
  slice(1000001:1100000)
proba12 = 
  dta22_filter %>%
  slice(1100001:1200000)

proba13 = 
  dta22_filter %>%
  slice(1200001:1300000)



proba_list <- list()

# Number of slices
num_slices <- 13

# Loop through each slice
for (i in 1:num_slices) {
  start <- (i - 1) * 100000 + 1
  end <- i * 100000
  
  # Create each sliced data frame and add it to the list
  proba_list[[i]] <- dta22_filter %>%
    slice(start:end)
}



```

```{r}
data22 = data22 %>%
  filter(SOURCE_TYPE == "web")




```


```{r}


setDT(dta22_filter)

dta22_filter[, FULL_TEXT := tolower(FULL_TEXT)]


generalno <- c("crkva", "biskup", "Kaptol", "časna", "sestra", "svećenik", "župnik", "vjernik", "kardinal", "papa", "sveti otac", "redovnik", "redovnica","kršćanstvo", "vjera", "Gospa", "Isus", "katolički", "misa", "pričest", "krizma", "grijeh", "vjeroučitelj", "vjeronauk", "blagoslov","svjedočanstvo", "relikvija", "stigma", "duhovnost", "velečasni","zaređenje", "krunica", "vjeronauk", "ukazanje","Stepinac", "HBK", "Opus Dei", "Caritas","vatikanski", "blagoslova","sakramenata", "prolife", "sekularizacija", "sekularna", "klerikalizam", "Crkva", "veličaju" ,"ustaštvo", "oduzimaju", "prava", "katoličku", "državu", "afera", "Kaptola",  "vjeronauk" ,"vatikanski", "klerikalna", "rodna" ,"ideologija", "klerikalizam" ,"brak", "blagoslovili" , "sekularna","Katolička", "pobačaj", "abortus",  "jezuiti", "prekrštavanje", "izopćen", "bludničio",  "posvećenje", "inkardiniran", "inkardinacija", "dogma", "tolerantna", "vjerska kontrola", "pedofil", "homoseksualnost", "patrijarhat", "ozdravljenje", "čudo") %>% tolower()
genralno_root <- sapply(generalno, write_tokens)
genralno_root <- sapply(strsplit(genralno_root, "\t"), `[`, 2)
generalno <- enframe(genralno_root, name = "name", value = "root")


words_vector <- str_c("(", str_c(generalno$root, collapse = "|"), ")")
words_vector <- str_c("\\b(", str_c(generalno$root, collapse = "|"), ")\\b")


# Vectorized function to check for matches
check_matches <- function(text, words_vector) {
  any(stri_detect_regex(text, words_vector, negate = FALSE))
}


batch_size <- 1000

# Calculate the number of batches
num_batches <- ceiling(nrow(dta22_filter) / batch_size)


# Loop through each batch
for (i in 1:num_batches) {
  
  start_time <- Sys.time()
  
  # Calculate the start and end row indices for the current batch
  start_idx <- (i - 1) * batch_size + 1
  end_idx <- min(i * batch_size, nrow(dta22_filter))
  
  # Print the current batch number and row indices
  cat(sprintf("Processing batch %d (rows %d to %d)...\n", i, start_idx, end_idx))
  
  # Subset the data table for the current batch and apply the operations
  dta22_filter[start_idx:end_idx, `:=` (
    has_word = sapply(FULL_TEXT, check_matches, words_vector),
    matched_word = sapply(FULL_TEXT, function(x) paste(unlist(str_extract_all(x, words_vector)), collapse=", "))
  )]
  
  batch_data <- dta22_filter[start_idx:end_idx]
  
  end_time <- Sys.time()
  duration <- end_time - start_time
  
  # Print the duration for the current batch
  cat(sprintf("Batch %d processed in %f seconds.\n", i, duration))
  
  # ... [rest of your loop code for saving etc.] ...
}


catoliq22 <- dta22_filter[has_word==T,]


rbind(catoliq23_1, catoliq23_2, catoliq23_3, catoliq23_4, catoliq23_5, catoliq23_6, catoliq23_7, catoliq23_8, catoliq23_9, catoliq23_10, catoliq23_11, catoliq23_12, catoliq23_13) -> catoliq23



write.xlsx(catoliq22, "C:/Users/lukas/Dropbox/HKS/Projekti/Dezinformacije/Data/Inicjalno citanje/catoliq22_not_top_25.xlsx")



```